batch  tensor([0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1], device='cuda:0')  output_vals  tensor([ 0.1020,  0.0120, -0.0048,  0.0078, -0.1323,  0.0041,  0.1880,  0.1696,
         0.1228,  0.1935,  0.1457, -0.0616, -0.2376,  0.3142, -0.2256,  0.0565,
        -0.2464, -0.2859,  0.0573,  0.0421,  0.1094, -0.0426, -0.1717,  0.1613,
         0.2601, -0.0477, -0.0888,  0.1108,  0.0635,  0.0240], device='cuda:0',
       grad_fn=<SliceBackward0>)  cot_outputs  tensor([-0.5000, -0.5000, -0.5000, -1.5000,  0.5000,  0.5000,  0.5000,  1.5000,
         1.5000,  1.5000, -1.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,
        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,
        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000], device='cuda:0')
Step 0 Loss: 0.8135102987289429 Final Loss: 0.8135102987289429
batch  tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0], device='cuda:0')  output_vals  tensor([ 0.0698,  0.7639,  0.3608, -0.7214, -0.4107,  0.7041, -0.6266, -0.5844,
        -0.5361, -0.3941, -0.4309,  0.2512,  0.5366,  0.8145,  0.5010,  0.7312,
         0.6180,  0.5524,  0.7726,  0.7611,  0.7316,  0.7112,  0.7203,  0.7821,
         0.8442,  0.7671,  0.7247,  0.8589,  0.8752,  0.9118], device='cuda:0',
       grad_fn=<SliceBackward0>)  cot_outputs  tensor([ 1.5000,  0.5000, -1.5000, -0.5000,  1.5000, -1.5000, -1.5000, -0.5000,
        -0.5000, -0.5000,  0.5000,  1.5000,  1.5000,  1.5000,  1.5000,  1.5000,
         1.5000,  1.5000,  1.5000,  1.5000,  1.5000,  1.5000,  1.5000,  1.5000,
         1.5000,  1.5000,  1.5000,  1.5000,  1.5000,  1.5000], device='cuda:0')
Step 100 Loss: 0.17221064865589142 Final Loss: 0.17221064865589142
batch  tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1], device='cuda:0')  output_vals  tensor([ 0.0640,  0.8578,  0.7169, -0.3395, -0.3714, -0.3247,  0.3348,  0.5451,
         0.3157,  0.4801,  0.4221,  0.3176,  0.6697,  0.9615,  0.6325,  0.8382,
         0.7519,  0.6932,  0.9297,  0.9176,  0.8002,  0.8106,  0.8502,  0.8840,
         0.9153,  0.9329,  0.8631,  0.9500,  0.9751,  1.1765], device='cuda:0',
       grad_fn=<SliceBackward0>)  cot_outputs  tensor([ 1.5000,  1.5000, -0.5000, -0.5000, -0.5000,  0.5000,  0.5000,  0.5000,
         0.5000,  0.5000,  0.5000,  1.5000,  1.5000,  1.5000,  1.5000,  1.5000,
         1.5000,  1.5000,  1.5000,  1.5000,  1.5000,  1.5000,  1.5000,  1.5000,
         1.5000,  1.5000,  1.5000,  1.5000,  1.5000,  1.5000], device='cuda:0')
Step 200 Loss: 0.05524279549717903 Final Loss: 0.05524279549717903
batch  tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1], device='cuda:0')  output_vals  tensor([ 0.0454, -0.3249, -0.6623,  0.3410,  0.7837,  0.7223,  0.7366,  0.7546,
         0.7517,  0.8275,  0.7933,  0.6785,  0.6228,  0.9453,  0.6026,  0.8158,
         0.6845,  0.6615,  0.9141,  0.8907,  0.7471,  0.7336,  0.7802,  0.8768,
         0.8482,  0.9084,  0.8309,  0.9361,  0.9150,  1.2659], device='cuda:0',
       grad_fn=<SliceBackward0>)  cot_outputs  tensor([-0.5000, -1.5000,  0.5000,  1.5000,  1.5000,  1.5000,  1.5000,  1.5000,
         1.5000,  1.5000,  1.5000,  1.5000,  1.5000,  1.5000,  1.5000,  1.5000,
         1.5000,  1.5000,  1.5000,  1.5000,  1.5000,  1.5000,  1.5000,  1.5000,
         1.5000,  1.5000,  1.5000,  1.5000,  1.5000,  1.5000], device='cuda:0')
Step 300 Loss: 0.02505531534552574 Final Loss: 0.02505531534552574
batch  tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 0], device='cuda:0')  output_vals  tensor([ 0.0760, -0.2570, -0.2403,  0.6398,  0.4206, -0.6327, -0.6144, -0.5394,
        -0.7612, -0.7290, -0.7592,  0.5824,  0.5240,  0.8885,  0.5209,  0.7832,
         0.6419,  0.5789,  0.8761,  0.8527,  0.7173,  0.6992,  0.7105,  0.8403,
         0.8362,  0.8548,  0.7602,  0.9356,  0.9045,  1.3139], device='cuda:0',
       grad_fn=<SliceBackward0>)  cot_outputs  tensor([-0.5000, -0.5000,  1.5000,  0.5000, -1.5000, -1.5000, -1.5000, -1.5000,
        -1.5000, -1.5000,  1.5000,  1.5000,  1.5000,  1.5000,  1.5000,  1.5000,
         1.5000,  1.5000,  1.5000,  1.5000,  1.5000,  1.5000,  1.5000,  1.5000,
         1.5000,  1.5000,  1.5000,  1.5000,  1.5000,  1.5000], device='cuda:0')
Step 400 Loss: 0.016125373542308807 Final Loss: 0.016125373542308807
batch  tensor([1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1], device='cuda:0')  output_vals  tensor([ 0.0732,  0.7310,  0.3103,  0.3381,  0.4199,  0.3307, -0.2387, -0.0574,
        -0.4295, -0.3099, -0.2915,  0.2607,  0.1139,  0.6590,  0.1170,  0.3095,
        -0.0422, -0.0028,  0.4727,  0.5140,  0.3348,  0.1837,  0.1791,  0.5298,
         0.4544,  0.5019,  0.2353,  0.4479,  0.4120,  0.5146], device='cuda:0',
       grad_fn=<SliceBackward0>)  cot_outputs  tensor([ 1.5000,  0.5000,  0.5000,  0.5000,  0.5000, -0.5000, -0.5000, -0.5000,
        -0.5000, -0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,
         0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,
         0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000], device='cuda:0')
Step 500 Loss: 0.00982683151960373 Final Loss: 0.00982683151960373
batch  tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0], device='cuda:0')  output_vals  tensor([ 0.0748,  0.7391,  0.6731,  0.6839,  0.8315,  0.7647, -0.5800, -0.5077,
        -0.7351, -0.7099, -0.7402,  0.6389,  0.1115,  0.6678,  0.1186,  0.3153,
        -0.0475, -0.0105,  0.4831,  0.5191,  0.3412,  0.1859,  0.1817,  0.5444,
         0.4613,  0.5108,  0.2364,  0.4518,  0.4193,  0.4964], device='cuda:0',
       grad_fn=<SliceBackward0>)  cot_outputs  tensor([ 1.5000,  1.5000,  1.5000,  1.5000,  1.5000, -1.5000, -1.5000, -1.5000,
        -1.5000, -1.5000,  1.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,
         0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,
         0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000], device='cuda:0')
Step 600 Loss: 0.009001949802041054 Final Loss: 0.009001949802041054
batch  tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1], device='cuda:0')  output_vals  tensor([ 0.0821, -0.2594, -0.6445,  0.3299,  0.8021, -0.2584,  0.2886,  0.4737,
         0.2179,  0.3999,  0.3576,  0.2689,  0.1164,  0.6788,  0.1261,  0.3206,
        -0.0430, -0.0063,  0.4885,  0.5262,  0.3480,  0.1908,  0.1840,  0.5509,
         0.4726,  0.5174,  0.2424,  0.4563,  0.4250,  0.4918], device='cuda:0',
       grad_fn=<SliceBackward0>)  cot_outputs  tensor([-0.5000, -1.5000,  0.5000,  1.5000, -0.5000,  0.5000,  0.5000,  0.5000,
         0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,
         0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,
         0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000], device='cuda:0')
Step 700 Loss: 0.006325447000563145 Final Loss: 0.006325447000563145
batch  tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0], device='cuda:0')  output_vals  tensor([ 0.0758,  0.7560,  0.6894,  0.7061,  0.8566, -0.2389,  0.3103,  0.4977,
         0.2343,  0.4216,  0.3723,  0.2905,  0.1277,  0.6891,  0.1479,  0.3426,
        -0.0284,  0.0040,  0.4994,  0.5336,  0.3565,  0.1998,  0.1945,  0.5662,
         0.4782,  0.5321,  0.2559,  0.4669,  0.4277,  0.4984], device='cuda:0',
       grad_fn=<SliceBackward0>)  cot_outputs  tensor([ 1.5000,  1.5000,  1.5000,  1.5000, -0.5000,  0.5000,  0.5000,  0.5000,
         0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,
         0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,
         0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000], device='cuda:0')
Step 800 Loss: 0.003239972982555628 Final Loss: 0.003239972982555628
batch  tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0], device='cuda:0')  output_vals  tensor([ 0.0852,  0.7731,  0.3275, -0.6710, -0.8446, -0.6749, -0.6742, -0.5704,
        -0.4882, -0.3599, -0.3285, -0.3478, -0.4838,  0.0704, -0.4795, -0.3792,
        -0.7534, -0.6613, -0.3119, -0.2232, -0.2511, -0.4832, -0.4610, -0.0511,
        -0.1724, -0.2431, -0.5969, -0.3285, -0.3743, -0.4939], device='cuda:0',
       grad_fn=<SliceBackward0>)  cot_outputs  tensor([ 1.5000,  0.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -0.5000,
        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,
        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,
        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000], device='cuda:0')
Step 900 Loss: 0.0028336881659924984 Final Loss: 0.0028336881659924984
batch  tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0], device='cuda:0')  output_vals  tensor([ 0.0861, -0.2654, -0.6705, -0.7087, -0.3581, -0.2737, -0.2905, -0.0852,
        -0.4964, -0.3671, -0.3307, -0.3524, -0.4884,  0.0689, -0.4854, -0.3865,
        -0.7614, -0.6680, -0.3166, -0.2290, -0.2539, -0.4892, -0.4655, -0.0510,
        -0.1761, -0.2465, -0.6031, -0.3343, -0.3803, -0.4951], device='cuda:0',
       grad_fn=<SliceBackward0>)  cot_outputs  tensor([-0.5000, -1.5000, -1.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,
        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,
        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,
        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000], device='cuda:0')
Step 1000 Loss: 0.0016389671945944428 Final Loss: 0.0016389671945944428
batch  tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0], device='cuda:0')  output_vals  tensor([ 0.0989, -0.2505, -0.2502,  0.7069,  0.4619, -0.6683,  0.7680,  0.8156,
         0.2490,  0.4350,  0.3984, -0.3097, -0.4496,  0.1104, -0.4450, -0.3224,
        -0.7298, -0.6397, -0.2667, -0.1908, -0.2121, -0.4477, -0.4404, -0.0128,
        -0.1325, -0.2223, -0.5833, -0.3176, -0.3480, -0.5027], device='cuda:0',
       grad_fn=<SliceBackward0>)  cot_outputs  tensor([-0.5000, -0.5000,  1.5000,  0.5000, -1.5000,  1.5000,  1.5000,  0.5000,
         0.5000,  0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,
        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,
        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000], device='cuda:0')
Step 1100 Loss: 0.0010108968708664179 Final Loss: 0.0010108968708664179
batch  tensor([1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1], device='cuda:0')  output_vals  tensor([ 0.1021,  0.7994,  0.7315, -0.2114, -0.8554,  0.3696, -0.2583, -0.0508,
        -0.8070, -0.7721, -0.7966,  0.6668,  0.1247,  0.7080,  0.1321,  0.3419,
        -0.0372, -0.0028,  0.5230,  0.5571,  0.3772,  0.2075,  0.2035,  0.5887,
         0.4987,  0.5497,  0.2594,  0.4799,  0.4523,  0.5008], device='cuda:0',
       grad_fn=<SliceBackward0>)  cot_outputs  tensor([ 1.5000,  1.5000, -0.5000, -1.5000,  0.5000, -0.5000, -0.5000, -1.5000,
        -1.5000, -1.5000,  1.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,
         0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,
         0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000], device='cuda:0')
Step 1200 Loss: 0.00045819769729860127 Final Loss: 0.00045819769729860127
batch  tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0], device='cuda:0')  output_vals  tensor([ 0.0976, -0.2556, -0.2552, -0.2311, -0.8803,  0.3556, -0.2804, -0.0681,
        -0.8355, -0.7973, -0.8240, -0.7559, -0.4918,  0.0651, -0.4897, -0.3990,
        -0.7746, -0.6755, -0.3215, -0.2350, -0.2543, -0.4946, -0.4687, -0.0489,
        -0.1798, -0.2464, -0.6105, -0.3493, -0.3882, -0.5070], device='cuda:0',
       grad_fn=<SliceBackward0>)  cot_outputs  tensor([-0.5000, -0.5000, -0.5000, -1.5000,  0.5000, -0.5000, -0.5000, -1.5000,
        -1.5000, -1.5000, -1.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,
        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,
        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000], device='cuda:0')
Step 1300 Loss: 0.00034813437378033996 Final Loss: 0.00034813437378033996
batch  tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1], device='cuda:0')  output_vals  tensor([ 0.1050, -0.2554, -0.2544,  0.7214,  0.4716,  0.3782,  0.3205,  0.5287,
         0.2513,  0.4402,  0.4051,  0.3075,  0.6488,  1.0201,  0.6517,  0.9189,
         0.7625,  0.7095,  1.0066,  0.9854,  0.8288,  0.7994,  0.8311,  0.9761,
         0.9501,  0.9934,  0.8997,  1.0731,  1.0212,  1.4850], device='cuda:0',
       grad_fn=<SliceBackward0>)  cot_outputs  tensor([-0.5000, -0.5000,  1.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,
         0.5000,  0.5000,  0.5000,  1.5000,  1.5000,  1.5000,  1.5000,  1.5000,
         1.5000,  1.5000,  1.5000,  1.5000,  1.5000,  1.5000,  1.5000,  1.5000,
         1.5000,  1.5000,  1.5000,  1.5000,  1.5000,  1.5000], device='cuda:0')
Step 1400 Loss: 0.00019559885549824685 Final Loss: 0.00019559885549824685
batch  tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1], device='cuda:0')  output_vals  tensor([ 0.0993, -0.2645, -0.6861, -0.7240, -0.3602,  0.7889,  0.7781,  0.8171,
         0.7940,  0.8851,  0.8677,  0.7293,  0.1505,  0.7360,  0.1690,  0.3680,
        -0.0141,  0.0181,  0.5460,  0.5828,  0.3938,  0.2247,  0.2220,  0.6137,
         0.5167,  0.5778,  0.2853,  0.4937,  0.4604,  0.4982], device='cuda:0',
       grad_fn=<SliceBackward0>)  cot_outputs  tensor([-0.5000, -1.5000, -1.5000, -0.5000,  1.5000,  1.5000,  1.5000,  1.5000,
         1.5000,  1.5000,  1.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,
         0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,
         0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000], device='cuda:0')
Step 1500 Loss: 7.845957588870078e-05 Final Loss: 7.845957588870078e-05
batch  tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1], device='cuda:0')  output_vals  tensor([ 0.0975, -0.2667, -0.2639, -0.2434, -0.3517,  0.7935, -0.6955, -0.5827,
        -0.4980, -0.3674, -0.3263, -0.3544, -0.4962,  0.0732, -0.4890, -0.3899,
        -0.7756, -0.6841, -0.3160, -0.2311, -0.2523, -0.4918, -0.4712, -0.0439,
        -0.1769, -0.2464, -0.6186, -0.3481, -0.3913, -0.4959], device='cuda:0',
       grad_fn=<SliceBackward0>)  cot_outputs  tensor([-0.5000, -0.5000, -0.5000, -0.5000,  1.5000, -1.5000, -1.5000, -0.5000,
        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,
        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,
        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000], device='cuda:0')
Step 1600 Loss: 4.370862734504044e-05 Final Loss: 4.370862734504044e-05
batch  tensor([0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1], device='cuda:0')  output_vals  tensor([ 0.1019, -0.2637, -0.6877, -0.7249, -0.9055,  0.3526, -0.3008, -0.0812,
        -0.8628, -0.8237, -0.8482,  0.6512,  0.5909,  0.9845,  0.5933,  0.8995,
         0.7192,  0.6594,  0.9838,  0.9572,  0.8135,  0.7845,  0.8040,  0.9594,
         0.9410,  0.9674,  0.8656,  1.0675,  1.0205,  1.4905], device='cuda:0',
       grad_fn=<SliceBackward0>)  cot_outputs  tensor([-0.5000, -1.5000, -1.5000, -1.5000,  0.5000, -0.5000, -0.5000, -1.5000,
        -1.5000, -1.5000,  1.5000,  1.5000,  1.5000,  1.5000,  1.5000,  1.5000,
         1.5000,  1.5000,  1.5000,  1.5000,  1.5000,  1.5000,  1.5000,  1.5000,
         1.5000,  1.5000,  1.5000,  1.5000,  1.5000,  1.5000], device='cuda:0')
Step 1700 Loss: 3.763458153116517e-05 Final Loss: 3.763458153116517e-05
batch  tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0], device='cuda:0')  output_vals  tensor([ 0.0942, -0.2673, -0.2644, -0.2454, -0.8928, -0.7169, -0.7284, -0.6070,
        -0.8767, -0.8383, -0.8618, -0.7888, -0.8865, -0.5968, -0.8723, -0.8683,
        -1.1355, -1.0359, -0.9706, -0.8838, -0.7873, -0.9718, -0.9639, -0.7103,
        -0.7412, -0.9006, -1.0767, -0.9786, -0.9341, -1.4907], device='cuda:0',
       grad_fn=<SliceBackward0>)  cot_outputs  tensor([-0.5000, -0.5000, -0.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000,
        -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000,
        -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000,
        -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000], device='cuda:0')
Step 1800 Loss: 2.4265809770440683e-05 Final Loss: 2.4265809770440683e-05
batch  tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 1], device='cuda:0')  output_vals  tensor([ 0.1083,  0.8169,  0.7494, -0.2106, -0.8675, -0.6865,  0.7878,  0.8326,
         0.2517,  0.4441,  0.4095, -0.3212, -0.8133, -0.4980, -0.8066, -0.7631,
        -1.0732, -0.9949, -0.8871, -0.8181, -0.7198, -0.9128, -0.9214, -0.6472,
        -0.6690, -0.8681, -1.0514, -0.9419, -0.8783, -1.5004], device='cuda:0',
       grad_fn=<SliceBackward0>)  cot_outputs  tensor([ 1.5000,  1.5000, -0.5000, -1.5000, -1.5000,  1.5000,  1.5000,  0.5000,
         0.5000,  0.5000, -0.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000,
        -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000,
        -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000], device='cuda:0')
Step 1900 Loss: 1.4893727893650066e-05 Final Loss: 1.4893727893650066e-05
Traceback (most recent call last):
  File "/accounts/projects/peter/gatmiry/distributional-CoT/train.py", line 51, in <module>
    main()
    ~~~~^^
  File "/accounts/projects/peter/gatmiry/.local/lib/python3.13/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
    ~~~~~~~~~~^
        args=args,
        ^^^^^^^^^^
    ...<3 lines>...
        config_name=config_name,
        ^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/accounts/projects/peter/gatmiry/.local/lib/python3.13/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
    ~~~~~~~~^
        run=args.run,
        ^^^^^^^^^^^^^
    ...<5 lines>...
        overrides=overrides,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/accounts/projects/peter/gatmiry/.local/lib/python3.13/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
    ~~~~~~~~~~~~~~^
        lambda: hydra.run(
        ^^^^^^^^^^^^^^^^^^
    ...<3 lines>...
        )
        ^
    )
    ^
  File "/accounts/projects/peter/gatmiry/.local/lib/python3.13/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/accounts/projects/peter/gatmiry/.local/lib/python3.13/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
            ~~~~~~~~~^
        config_name=config_name,
        ^^^^^^^^^^^^^^^^^^^^^^^^
        task_function=task_function,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        overrides=overrides,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/accounts/projects/peter/gatmiry/.local/lib/python3.13/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
        hydra_context=HydraContext(
    ...<6 lines>...
        configure_logging=with_log_configuration,
    )
  File "/accounts/projects/peter/gatmiry/.local/lib/python3.13/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
                       ~~~~~~~~~~~~~^^^^^^^^^^
  File "/accounts/projects/peter/gatmiry/distributional-CoT/train.py", line 47, in main
    train.fit()
    ~~~~~~~~~^^
  File "/accounts/projects/peter/gatmiry/distributional-CoT/test_composite_function.py", line 79, in fit
    batch, cot_outputs = dataset.get_batch(batch_size=batch_size, flipping_subsets=flipping_subsets)
                         ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/accounts/projects/peter/gatmiry/distributional-CoT/test_composite_function.py", line 239, in get_batch
    result = self.target_function(cot_tmp_batch_flipping)
  File "/accounts/projects/peter/gatmiry/distributional-CoT/test_composite_function.py", line 197, in compute_function
    term_vals = term_vals * samples[:, var]
    ^^^^^^^^^
KeyboardInterrupt
