[34m[1mwandb[0m: Detected [huggingface_hub.inference, openai] in use.
[34m[1mwandb[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
[34m[1mwandb[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/
                                                                                                                                         
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 4.4331, 'grad_norm': 142.0, 'learning_rate': 0.0, 'epoch': 0.0}
{'loss': 4.3839, 'grad_norm': 143.0, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.0}
{'loss': 4.3155, 'grad_norm': 137.0, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.0}
{'loss': 3.8428, 'grad_norm': 130.0, 'learning_rate': 1.2e-05, 'epoch': 0.01}
{'loss': 3.3159, 'grad_norm': 126.0, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.01}
{'loss': 2.1949, 'grad_norm': 122.0, 'learning_rate': 2e-05, 'epoch': 0.01}
{'loss': 1.5527, 'grad_norm': 70.5, 'learning_rate': 1.9974259974259977e-05, 'epoch': 0.01}
{'loss': 1.0764, 'grad_norm': 52.75, 'learning_rate': 1.994851994851995e-05, 'epoch': 0.01}
{'loss': 0.736, 'grad_norm': 37.25, 'learning_rate': 1.9922779922779923e-05, 'epoch': 0.01}
{'loss': 0.5394, 'grad_norm': 24.875, 'learning_rate': 1.9897039897039898e-05, 'epoch': 0.01}
{'loss': 0.4292, 'grad_norm': 16.5, 'learning_rate': 1.9871299871299873e-05, 'epoch': 0.01}
{'loss': 0.3745, 'grad_norm': 10.4375, 'learning_rate': 1.9845559845559848e-05, 'epoch': 0.02}
{'loss': 0.3207, 'grad_norm': 5.40625, 'learning_rate': 1.981981981981982e-05, 'epoch': 0.02}
{'loss': 0.3093, 'grad_norm': 6.03125, 'learning_rate': 1.9794079794079795e-05, 'epoch': 0.02}
{'loss': 0.2955, 'grad_norm': 8.9375, 'learning_rate': 1.976833976833977e-05, 'epoch': 0.02}
{'loss': 0.2892, 'grad_norm': 5.03125, 'learning_rate': 1.9742599742599745e-05, 'epoch': 0.02}
{'loss': 0.3086, 'grad_norm': 6.1875, 'learning_rate': 1.9716859716859716e-05, 'epoch': 0.02}
{'loss': 0.2912, 'grad_norm': 10.875, 'learning_rate': 1.9691119691119695e-05, 'epoch': 0.02}
{'loss': 0.2998, 'grad_norm': 4.5625, 'learning_rate': 1.9665379665379666e-05, 'epoch': 0.02}
{'loss': 0.2938, 'grad_norm': 4.875, 'learning_rate': 1.963963963963964e-05, 'epoch': 0.03}
{'loss': 0.2919, 'grad_norm': 4.46875, 'learning_rate': 1.9613899613899616e-05, 'epoch': 0.03}
{'loss': 0.2935, 'grad_norm': 4.9375, 'learning_rate': 1.958815958815959e-05, 'epoch': 0.03}
{'loss': 0.3033, 'grad_norm': 4.0625, 'learning_rate': 1.9562419562419563e-05, 'epoch': 0.03}
{'loss': 0.2884, 'grad_norm': 10.3125, 'learning_rate': 1.9536679536679538e-05, 'epoch': 0.03}
{'loss': 0.2781, 'grad_norm': 3.96875, 'learning_rate': 1.9510939510939513e-05, 'epoch': 0.03}
{'loss': 0.2892, 'grad_norm': 8.75, 'learning_rate': 1.9485199485199485e-05, 'epoch': 0.03}
{'loss': 0.2808, 'grad_norm': 9.125, 'learning_rate': 1.9459459459459463e-05, 'epoch': 0.03}
{'loss': 0.2735, 'grad_norm': 3.0625, 'learning_rate': 1.9433719433719435e-05, 'epoch': 0.04}
{'loss': 0.2967, 'grad_norm': 7.0, 'learning_rate': 1.940797940797941e-05, 'epoch': 0.04}
{'loss': 0.2954, 'grad_norm': 6.875, 'learning_rate': 1.938223938223938e-05, 'epoch': 0.04}
{'loss': 0.2757, 'grad_norm': 2.984375, 'learning_rate': 1.935649935649936e-05, 'epoch': 0.04}
{'loss': 0.2908, 'grad_norm': 6.3125, 'learning_rate': 1.933075933075933e-05, 'epoch': 0.04}
{'loss': 0.2937, 'grad_norm': 4.8125, 'learning_rate': 1.9305019305019306e-05, 'epoch': 0.04}
{'loss': 0.2863, 'grad_norm': 5.375, 'learning_rate': 1.927927927927928e-05, 'epoch': 0.04}
{'loss': 0.2873, 'grad_norm': 3.453125, 'learning_rate': 1.9253539253539256e-05, 'epoch': 0.04}
{'loss': 0.2924, 'grad_norm': 3.625, 'learning_rate': 1.9227799227799228e-05, 'epoch': 0.05}
{'loss': 0.2988, 'grad_norm': 6.65625, 'learning_rate': 1.9202059202059203e-05, 'epoch': 0.05}
{'loss': 0.282, 'grad_norm': 3.734375, 'learning_rate': 1.9176319176319178e-05, 'epoch': 0.05}
{'loss': 0.2956, 'grad_norm': 3.640625, 'learning_rate': 1.9150579150579153e-05, 'epoch': 0.05}
{'loss': 0.2917, 'grad_norm': 5.40625, 'learning_rate': 1.9124839124839128e-05, 'epoch': 0.05}
{'loss': 0.2813, 'grad_norm': 4.3125, 'learning_rate': 1.90990990990991e-05, 'epoch': 0.05}
{'loss': 0.2878, 'grad_norm': 3.328125, 'learning_rate': 1.9073359073359075e-05, 'epoch': 0.05}
{'loss': 0.2919, 'grad_norm': 4.21875, 'learning_rate': 1.904761904761905e-05, 'epoch': 0.06}
{'loss': 0.2919, 'grad_norm': 6.0, 'learning_rate': 1.9021879021879024e-05, 'epoch': 0.06}
{'loss': 0.2908, 'grad_norm': 3.6875, 'learning_rate': 1.8996138996138996e-05, 'epoch': 0.06}
{'loss': 0.2936, 'grad_norm': 4.21875, 'learning_rate': 1.897039897039897e-05, 'epoch': 0.06}
{'loss': 0.282, 'grad_norm': 3.34375, 'learning_rate': 1.8944658944658946e-05, 'epoch': 0.06}
{'loss': 0.278, 'grad_norm': 4.03125, 'learning_rate': 1.891891891891892e-05, 'epoch': 0.06}
{'loss': 0.2795, 'grad_norm': 3.46875, 'learning_rate': 1.8893178893178893e-05, 'epoch': 0.06}
{'loss': 0.2891, 'grad_norm': 5.59375, 'learning_rate': 1.886743886743887e-05, 'epoch': 0.06}
{'loss': 0.2831, 'grad_norm': 2.875, 'learning_rate': 1.8841698841698843e-05, 'epoch': 0.07}
{'loss': 0.2807, 'grad_norm': 3.609375, 'learning_rate': 1.8815958815958818e-05, 'epoch': 0.07}
{'loss': 0.2824, 'grad_norm': 5.1875, 'learning_rate': 1.8790218790218793e-05, 'epoch': 0.07}
{'loss': 0.293, 'grad_norm': 8.6875, 'learning_rate': 1.8764478764478768e-05, 'epoch': 0.07}
{'loss': 0.2904, 'grad_norm': 5.4375, 'learning_rate': 1.873873873873874e-05, 'epoch': 0.07}
{'loss': 0.2877, 'grad_norm': 4.6875, 'learning_rate': 1.8712998712998714e-05, 'epoch': 0.07}
{'loss': 0.2838, 'grad_norm': 3.75, 'learning_rate': 1.868725868725869e-05, 'epoch': 0.07}
{'loss': 0.2864, 'grad_norm': 6.6875, 'learning_rate': 1.8661518661518664e-05, 'epoch': 0.07}
{'loss': 0.2832, 'grad_norm': 2.84375, 'learning_rate': 1.8635778635778636e-05, 'epoch': 0.08}
{'loss': 0.277, 'grad_norm': 2.859375, 'learning_rate': 1.861003861003861e-05, 'epoch': 0.08}
{'loss': 0.2906, 'grad_norm': 3.03125, 'learning_rate': 1.8584298584298586e-05, 'epoch': 0.08}
{'loss': 0.2878, 'grad_norm': 6.46875, 'learning_rate': 1.855855855855856e-05, 'epoch': 0.08}
{'loss': 0.2771, 'grad_norm': 4.09375, 'learning_rate': 1.8532818532818536e-05, 'epoch': 0.08}
{'loss': 0.2851, 'grad_norm': 2.78125, 'learning_rate': 1.8507078507078508e-05, 'epoch': 0.08}
{'loss': 0.2669, 'grad_norm': 4.15625, 'learning_rate': 1.8481338481338483e-05, 'epoch': 0.08}
{'loss': 0.2821, 'grad_norm': 3.0, 'learning_rate': 1.8455598455598458e-05, 'epoch': 0.08}
{'loss': 0.2948, 'grad_norm': 6.8125, 'learning_rate': 1.8429858429858433e-05, 'epoch': 0.09}
{'loss': 0.2671, 'grad_norm': 3.71875, 'learning_rate': 1.8404118404118404e-05, 'epoch': 0.09}
{'loss': 0.3003, 'grad_norm': 7.59375, 'learning_rate': 1.8378378378378383e-05, 'epoch': 0.09}
{'loss': 0.2925, 'grad_norm': 9.25, 'learning_rate': 1.8352638352638354e-05, 'epoch': 0.09}
{'loss': 0.2654, 'grad_norm': 5.125, 'learning_rate': 1.832689832689833e-05, 'epoch': 0.09}
{'loss': 0.2769, 'grad_norm': 2.6875, 'learning_rate': 1.83011583011583e-05, 'epoch': 0.09}
{'loss': 0.2873, 'grad_norm': 4.375, 'learning_rate': 1.8275418275418276e-05, 'epoch': 0.09}
{'loss': 0.278, 'grad_norm': 5.90625, 'learning_rate': 1.824967824967825e-05, 'epoch': 0.09}
{'loss': 0.2769, 'grad_norm': 2.796875, 'learning_rate': 1.8223938223938226e-05, 'epoch': 0.1}
{'loss': 0.2843, 'grad_norm': 4.46875, 'learning_rate': 1.81981981981982e-05, 'epoch': 0.1}
{'loss': 0.2845, 'grad_norm': 2.765625, 'learning_rate': 1.8172458172458172e-05, 'epoch': 0.1}
{'loss': 0.2889, 'grad_norm': 4.09375, 'learning_rate': 1.8146718146718147e-05, 'epoch': 0.1}
{'loss': 0.2876, 'grad_norm': 2.515625, 'learning_rate': 1.8120978120978122e-05, 'epoch': 0.1}
{'loss': 0.2835, 'grad_norm': 7.75, 'learning_rate': 1.8095238095238097e-05, 'epoch': 0.1}
{'loss': 0.2878, 'grad_norm': 5.28125, 'learning_rate': 1.806949806949807e-05, 'epoch': 0.1}
{'loss': 0.2886, 'grad_norm': 4.03125, 'learning_rate': 1.8043758043758047e-05, 'epoch': 0.1}
{'loss': 0.2985, 'grad_norm': 3.640625, 'learning_rate': 1.801801801801802e-05, 'epoch': 0.11}
{'loss': 0.2841, 'grad_norm': 3.90625, 'learning_rate': 1.7992277992277994e-05, 'epoch': 0.11}
{'loss': 0.2776, 'grad_norm': 2.859375, 'learning_rate': 1.796653796653797e-05, 'epoch': 0.11}
{'loss': 0.2983, 'grad_norm': 6.34375, 'learning_rate': 1.7940797940797944e-05, 'epoch': 0.11}
{'loss': 0.287, 'grad_norm': 2.59375, 'learning_rate': 1.7915057915057916e-05, 'epoch': 0.11}
{'loss': 0.2867, 'grad_norm': 4.09375, 'learning_rate': 1.788931788931789e-05, 'epoch': 0.11}
{'loss': 0.2858, 'grad_norm': 3.09375, 'learning_rate': 1.7863577863577866e-05, 'epoch': 0.11}
{'loss': 0.2892, 'grad_norm': 3.03125, 'learning_rate': 1.783783783783784e-05, 'epoch': 0.12}
{'loss': 0.2874, 'grad_norm': 8.125, 'learning_rate': 1.7812097812097812e-05, 'epoch': 0.12}
{'loss': 0.2861, 'grad_norm': 5.28125, 'learning_rate': 1.7786357786357787e-05, 'epoch': 0.12}
{'loss': 0.2865, 'grad_norm': 5.5, 'learning_rate': 1.7760617760617762e-05, 'epoch': 0.12}
{'loss': 0.2735, 'grad_norm': 3.09375, 'learning_rate': 1.7734877734877737e-05, 'epoch': 0.12}
{'loss': 0.2742, 'grad_norm': 7.28125, 'learning_rate': 1.7709137709137712e-05, 'epoch': 0.12}
{'loss': 0.2768, 'grad_norm': 2.296875, 'learning_rate': 1.7683397683397684e-05, 'epoch': 0.12}
{'loss': 0.2867, 'grad_norm': 5.5625, 'learning_rate': 1.765765765765766e-05, 'epoch': 0.12}
{'loss': 0.2837, 'grad_norm': 5.8125, 'learning_rate': 1.7631917631917634e-05, 'epoch': 0.13}
{'loss': 0.2782, 'grad_norm': 2.5, 'learning_rate': 1.760617760617761e-05, 'epoch': 0.13}
{'loss': 0.2855, 'grad_norm': 3.109375, 'learning_rate': 1.758043758043758e-05, 'epoch': 0.13}
{'loss': 0.2964, 'grad_norm': 7.625, 'learning_rate': 1.7554697554697555e-05, 'epoch': 0.13}
{'loss': 0.2845, 'grad_norm': 3.4375, 'learning_rate': 1.752895752895753e-05, 'epoch': 0.13}
{'loss': 0.2888, 'grad_norm': 4.46875, 'learning_rate': 1.7503217503217505e-05, 'epoch': 0.13}
{'loss': 0.2598, 'grad_norm': 5.09375, 'learning_rate': 1.7477477477477477e-05, 'epoch': 0.13}
{'loss': 0.2872, 'grad_norm': 2.5, 'learning_rate': 1.7451737451737455e-05, 'epoch': 0.13}
{'loss': 0.2832, 'grad_norm': 3.890625, 'learning_rate': 1.7425997425997427e-05, 'epoch': 0.14}
{'loss': 0.2812, 'grad_norm': 3.921875, 'learning_rate': 1.7400257400257402e-05, 'epoch': 0.14}
{'loss': 0.2812, 'grad_norm': 2.59375, 'learning_rate': 1.7374517374517377e-05, 'epoch': 0.14}
{'loss': 0.2878, 'grad_norm': 4.15625, 'learning_rate': 1.734877734877735e-05, 'epoch': 0.14}
{'loss': 0.2801, 'grad_norm': 7.59375, 'learning_rate': 1.7323037323037324e-05, 'epoch': 0.14}
{'loss': 0.2857, 'grad_norm': 2.84375, 'learning_rate': 1.72972972972973e-05, 'epoch': 0.14}
{'loss': 0.2808, 'grad_norm': 3.53125, 'learning_rate': 1.7271557271557274e-05, 'epoch': 0.14}
{'loss': 0.2847, 'grad_norm': 3.859375, 'learning_rate': 1.7245817245817245e-05, 'epoch': 0.14}
{'loss': 0.292, 'grad_norm': 7.375, 'learning_rate': 1.722007722007722e-05, 'epoch': 0.15}
{'loss': 0.2754, 'grad_norm': 2.609375, 'learning_rate': 1.7194337194337195e-05, 'epoch': 0.15}
{'loss': 0.2804, 'grad_norm': 2.390625, 'learning_rate': 1.716859716859717e-05, 'epoch': 0.15}
  File "/accounts/projects/sewonm/prasann/projects/curriculum-CoT/biglmexps/finetune.py", line 72, in <module>
    trainerstats = trainer.train()
                   ^^^^^^^^^^^^^^^
  File "/accounts/projects/sewonm/prasann/projects/curriculum-CoT/unsloth_compiled_cache/UnslothSFTTrainer.py", line 64, in wrapper
    output = f(self, *args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/accounts/projects/sewonm/prasann/.conda/envs/cotenv/lib/python3.12/site-packages/transformers/trainer.py", line 2325, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 328, in _fast_inner_training_loop
  File "/accounts/projects/sewonm/prasann/projects/curriculum-CoT/unsloth_compiled_cache/UnslothSFTTrainer.py", line 1218, in training_step
    return super().training_step(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 91, in _unsloth_training_step
  File "/accounts/projects/sewonm/prasann/.conda/envs/cotenv/lib/python3.12/site-packages/accelerate/accelerator.py", line 2852, in backward
    loss.backward(**kwargs)
  File "/accounts/projects/sewonm/prasann/.conda/envs/cotenv/lib/python3.12/site-packages/torch/_tensor.py", line 625, in backward
    torch.autograd.backward(
  File "/accounts/projects/sewonm/prasann/.conda/envs/cotenv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 354, in backward
    _engine_run_backward(
  File "/accounts/projects/sewonm/prasann/.conda/envs/cotenv/lib/python3.12/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
