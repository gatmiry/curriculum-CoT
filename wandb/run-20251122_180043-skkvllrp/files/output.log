================================================================================
Using device: cuda
batch  tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 0], device='cuda:0')  output_vals  tensor([-0.0173, -0.2084, -0.1942, -0.1544, -0.1560, -0.1856, -0.1682, -0.2433,
        -0.2040, -0.0894], device='cuda:0', grad_fn=<SliceBackward0>)  cot_outputs  tensor([2., 2., 2., 2., 2., 2., 2., 2., 0., 0.], device='cuda:0')
Step 0 Loss: 3.7611026763916016 Final Loss: 0.041608210653066635
batch  tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 1], device='cuda:0')  output_vals  tensor([ 0.9838,  0.1564,  0.1615,  0.1156,  0.0956,  0.0474,  0.1118, -1.0487,
        -1.1612, -0.1515], device='cuda:0', grad_fn=<SliceBackward0>)  cot_outputs  tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0., -2.,  0.,  0.], device='cuda:0')
Step 100 Loss: 0.33318030834198 Final Loss: 1.3482859134674072
batch  tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 0], device='cuda:0')  output_vals  tensor([ 8.9053e-01,  1.3885e-02,  2.4378e-02,  1.2792e-02,  2.1074e-02,
         1.4084e-03, -2.1608e-03, -1.3228e+00, -1.4089e+00, -1.4149e+00],
       device='cuda:0', grad_fn=<SliceBackward0>)  cot_outputs  tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0., -2., -2., -2.], device='cuda:0')
Step 200 Loss: 0.1944737434387207 Final Loss: 0.34937480092048645
batch  tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 1], device='cuda:0')  output_vals  tensor([1.0549, 1.5933, 1.6198, 1.6244, 1.6180, 1.6248, 1.6157, 1.3872, 0.8535,
        0.1826], device='cuda:0', grad_fn=<SliceBackward0>)  cot_outputs  tensor([2., 2., 2., 2., 2., 2., 2., 0., 0., 0.], device='cuda:0')
Step 300 Loss: 0.44648218154907227 Final Loss: 0.7285365462303162
batch  tensor([1, 0, 1, 0, 1, 1, 0, 0, 0, 1], device='cuda:0')  output_vals  tensor([1.7792, 1.8086, 1.8185, 1.8088, 1.8081, 1.8062, 1.8065, 1.1246, 1.1352,
        1.7193], device='cuda:0', grad_fn=<SliceBackward0>)  cot_outputs  tensor([2., 2., 2., 2., 2., 2., 2., 0., 2., 2.], device='cuda:0')
Step 400 Loss: 0.23580197989940643 Final Loss: 0.7478393912315369
batch  tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 0], device='cuda:0')  output_vals  tensor([1.8767, 1.8804, 1.8999, 1.8919, 1.8949, 1.8941, 1.8898, 1.3061, 1.1181,
        0.1713], device='cuda:0', grad_fn=<SliceBackward0>)  cot_outputs  tensor([2., 2., 2., 2., 2., 2., 2., 0., 0., 0.], device='cuda:0')
Step 500 Loss: 0.30708879232406616 Final Loss: 1.2500842809677124
batch  tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0], device='cuda:0')  output_vals  tensor([1.9450, 1.9440, 1.9555, 1.9354, 1.9376, 1.9375, 1.9321, 0.9342, 1.0022,
        0.0731], device='cuda:0', grad_fn=<SliceBackward0>)  cot_outputs  tensor([2., 2., 2., 2., 2., 2., 2., 0., 0., 0.], device='cuda:0')
Step 600 Loss: 0.1907304972410202 Final Loss: 1.0044490098953247
batch  tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 1], device='cuda:0')  output_vals  tensor([1.9644, 1.9950, 1.9934, 1.9645, 1.9743, 1.9795, 1.9708, 0.8846, 1.3428,
        1.8170], device='cuda:0', grad_fn=<SliceBackward0>)  cot_outputs  tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.], device='cuda:0')
Step 700 Loss: 0.17140386998653412 Final Loss: 0.43195614218711853
batch  tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 0], device='cuda:0')  output_vals  tensor([1.9738, 2.0013, 2.0118, 1.9964, 2.0031, 2.0086, 1.9982, 1.1863, 1.2370,
        0.0478], device='cuda:0', grad_fn=<SliceBackward0>)  cot_outputs  tensor([2., 2., 2., 2., 2., 2., 2., 0., 0., 0.], device='cuda:0')
Step 800 Loss: 0.29406246542930603 Final Loss: 1.5301668643951416
batch  tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 1], device='cuda:0')  output_vals  tensor([1.9635, 1.9713, 2.0036, 1.9598, 1.9705, 1.9635, 1.9466, 0.8995, 0.5173,
        0.0240], device='cuda:0', grad_fn=<SliceBackward0>)  cot_outputs  tensor([2., 2., 2., 2., 2., 2., 2., 0., 0., 0.], device='cuda:0')
Step 900 Loss: 0.10860981792211533 Final Loss: 0.26756975054740906
batch  tensor([0, 1, 0, 0, 0, 1, 0, 1, 0, 0], device='cuda:0')  output_vals  tensor([-0.0109, -0.0286, -0.0173, -0.0189, -0.0072,  0.0068, -0.0071, -0.8752,
        -0.8064, -1.9228], device='cuda:0', grad_fn=<SliceBackward0>)  cot_outputs  tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -2., -2.], device='cuda:0')
Step 1000 Loss: 0.2198501080274582 Final Loss: 1.4247692823410034
Traceback (most recent call last):
  File "/accounts/projects/peter/gatmiry/distributional-CoT/train.py", line 51, in <module>
    main()
    ~~~~^^
  File "/accounts/projects/peter/gatmiry/.local/lib/python3.13/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
    ~~~~~~~~~~^
        args=args,
        ^^^^^^^^^^
    ...<3 lines>...
        config_name=config_name,
        ^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/accounts/projects/peter/gatmiry/.local/lib/python3.13/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
    ~~~~~~~~^
        run=args.run,
        ^^^^^^^^^^^^^
    ...<5 lines>...
        overrides=overrides,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/accounts/projects/peter/gatmiry/.local/lib/python3.13/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
    ~~~~~~~~~~~~~~^
        lambda: hydra.run(
        ^^^^^^^^^^^^^^^^^^
    ...<3 lines>...
        )
        ^
    )
    ^
  File "/accounts/projects/peter/gatmiry/.local/lib/python3.13/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/accounts/projects/peter/gatmiry/.local/lib/python3.13/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
            ~~~~~~~~~^
        config_name=config_name,
        ^^^^^^^^^^^^^^^^^^^^^^^^
        task_function=task_function,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        overrides=overrides,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/accounts/projects/peter/gatmiry/.local/lib/python3.13/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
        hydra_context=HydraContext(
    ...<6 lines>...
        configure_logging=with_log_configuration,
    )
  File "/accounts/projects/peter/gatmiry/.local/lib/python3.13/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
                       ~~~~~~~~~~~~~^^^^^^^^^^
  File "/accounts/projects/peter/gatmiry/distributional-CoT/train.py", line 47, in main
    train.fit()
    ~~~~~~~~~^^
  File "/accounts/projects/peter/gatmiry/distributional-CoT/test_composite_function.py", line 63, in fit
    output_vals, loss = model(batch, cot_outputs)
                        ~~~~~^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/linux/miniforge-3.13/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/usr/local/linux/miniforge-3.13/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/accounts/projects/peter/gatmiry/distributional-CoT/model.py", line 103, in forward
    x = block(x)
  File "/usr/local/linux/miniforge-3.13/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/usr/local/linux/miniforge-3.13/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/accounts/projects/peter/gatmiry/distributional-CoT/model.py", line 63, in forward
    x = x + self.c_attn(self.ln_1(x), last_k_no_attend=0, window_size=0)
            ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/linux/miniforge-3.13/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/usr/local/linux/miniforge-3.13/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/accounts/projects/peter/gatmiry/distributional-CoT/model.py", line 51, in forward
    y = self.c_proj(y)
  File "/usr/local/linux/miniforge-3.13/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/usr/local/linux/miniforge-3.13/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/linux/miniforge-3.13/lib/python3.13/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
